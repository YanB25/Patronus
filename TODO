[-] Add API p->should_exit()
[-] Add key_to_dsm_addr API for addressing.
[-] Bench the performance / coroutine.

[-] when QP fails, before recovery, we should wait for all the on-going contexts, and set result to false.
[-] then, we should return all the onging coro ids. so that, the client_master will yield to these coroutines
[-] then, all the worker will realize that they fail. so they all retries.
[-] refactor: the API for client should combine into one.

[-] implement relinquish, and let the server gc the memory window. so that I can bench for a looong time instead of several ms.
[-] try to optimize the performance of single-thread patronus-basic (takes 2-3 days if lucky). actually: I got sick and sleep for 2-3 days =).
[-] remember to measure the latency of each op, and the penalty of coroutine switching.

- then, try the correctness and performance of multiple threads. should scale according to NR_DIRECTORY, I guess.

[-] evaluate the maxinum bind_mw performance I can get:
    - What if pipeline poll cq? for the bench_mw.cpp, advancely post several batch.
    - What if totally does not poll? just try_poll and does not wait for anything.
    - tune up the post-send-batch for bench_mw_post_send.

[one day]
Possible performance optimization:
[-] performance of buffer pool? should be REALLY high: 52.6Mops for obj pool, 69.2Mops for buffer pool.
[-] server can bind_mw in a batch (from one ibv_post_send call). There will be only one signal(, or every signal) and one interaction to NIC.
    [-] server SHOULD NOT ASSUME that one server thread is bind to a DirectoryConnection.
    [-] NOT WORK: but server DOES can batch post_send and get rid of coroutine
    [-] NOT IMPL (feat/mw-server-not-use-coroutine): should test against the correctness of failed mw or failed r/w.
    [-] NOT IMPL (feat/mw-server-not-use-coroutine): use prepare_handle_request_acquire, and commit_handle_request_acquire internal API. that would be easy to adopt the codes.

[-] move the small function to patronus.h, I don't trust linker optimization so much.
[-] API change: do not expose server coroutine scheduling to the outer world. 
    [-] NOT WORK(no performance gain): Furthermore, no copy is required.
[-] evaluation: bench the latency of the whole procedure. with various concurrency. NOTE: because of a bug, this really took me hours! And I am so reluctant to work today. The result is at Yuque.


1th Three days DDL: 1/10-1/12
[-] evaluation: test what happen if issue multiple mw into the same wr! that would be cooool, if the batching does not lower down performance!
    - 1 mw get 1.5 Mops. 2 mw get 1.36 Mops. 4 mw get 1 Mops. 8 mw get 890 Kops.
[-] WORK, PASSED: should be a quick test: see if rmsg can work correctly, if the QP is used by ONE thread, but not SAME thread.
    if this luckly work well, then the design would be great: one client thread map to one server thread map to one Dir thread.
[-] DONE measure the overhead of boost coroutine: especially for large stack (std::array<1024>), or deep stack (recursive calls)
    - got 40 Mops.
    - boost::coroutine is stackful. runtime overhead only contains registers load/store. will not copy the whole stack. GREAT
    
- change to multiple threads: 
    - tune NR_DIRECTORY to 4
    - each client thread bind to one mid. (e.g. need 16 client threads)
    - one server thread can handle multiple mid simultaneously. one server thread for one DirectoryConnnection (one-to-one mapping). Therefore, 4 server threads.
- bench the performance. see if it is 4 times as one thread.
- Add the header into the design. That would require adding the swaping area, and makes everything works well.


Futures:
- Add the TIME into patronus. Server should intervally see if any expired lease exists, and destroy them!
- Implemente all the APIs in Patronus
If all the things above finish, I can move on to adopt the uPaxos!