[-] Add API p->should_exit()
[-] Add key_to_dsm_addr API for addressing.
[-] Bench the performance / coroutine.

[-] when QP fails, before recovery, we should wait for all the on-going contexts, and set result to false.
[-] then, we should return all the onging coro ids. so that, the client_master will yield to these coroutines
[-] then, all the worker will realize that they fail. so they all retries.
[-] refactor: the API for client should combine into one.

[-] implement relinquish, and let the server gc the memory window. so that I can bench for a looong time instead of several ms.
[-] try to optimize the performance of single-thread patronus-basic (takes 2-3 days if lucky). actually: I got sick and sleep for 2-3 days =).
[-] remember to measure the latency of each op, and the penalty of coroutine switching.

[-] then, try the correctness and performance of multiple threads. should scale according to NR_DIRECTORY, I guess.

[-] evaluate the maxinum bind_mw performance I can get:
    - What if pipeline poll cq? for the bench_mw.cpp, advancely post several batch.
    - What if totally does not poll? just try_poll and does not wait for anything.
    - tune up the post-send-batch for bench_mw_post_send.

[one day]
Possible performance optimization:
[-] performance of buffer pool? should be REALLY high: 52.6Mops for obj pool, 69.2Mops for buffer pool.
[-] server can bind_mw in a batch (from one ibv_post_send call). There will be only one signal(, or every signal) and one interaction to NIC.
    [-] server SHOULD NOT ASSUME that one server thread is bind to a DirectoryConnection.
    [-] NOT WORK: but server DOES can batch post_send and get rid of coroutine
    [-] NOT IMPL (feat/mw-server-not-use-coroutine): should test against the correctness of failed mw or failed r/w.
    [-] NOT IMPL (feat/mw-server-not-use-coroutine): use prepare_handle_request_acquire, and commit_handle_request_acquire internal API. that would be easy to adopt the codes.

[-] move the small function to patronus.h, I don't trust linker optimization so much.
[-] API change: do not expose server coroutine scheduling to the outer world. 
    [-] NOT WORK(no performance gain): Furthermore, no copy is required.
[-] evaluation: bench the latency of the whole procedure. with various concurrency. NOTE: because of a bug, this really took me hours! And I am so reluctant to work today. The result is at Yuque.


1st Three days DDL: 1/10-1/12
[-] evaluation: test what happen if issue multiple mw into the same wr! that would be cooool, if the batching does not lower down performance!
    - 1 mw get 1.5 Mops. 2 mw get 1.36 Mops. 4 mw get 1 Mops. 8 mw get 890 Kops.
[-] WORK, PASSED: should be a quick test: see if rmsg can work correctly, if the QP is used by ONE thread, but not SAME thread.
    if this luckly work well, then the design would be great: one client thread map to one server thread map to one Dir thread.
[-] DONE measure the overhead of boost coroutine: especially for large stack (std::array<1024>), or deep stack (recursive calls)
    - got 40 Mops.
    - boost::coroutine is stackful. runtime overhead only contains registers load/store. will not copy the whole stack. GREAT
[-] boost::coroutine is just DOES NOT WORK WITH ASAN. 
[-] DONE: change to multiple threads. result: the scalability is not bad. 1 thread get 1.5 Mops per thread. 8 threads get 1.16 Mops.
    - tune NR_DIRECTORY to 4. NOW: 8
    - each client thread bind to one mid. (e.g. need 16 client threads)
    - one server thread can handle multiple mid simultaneously. one server thread for one DirectoryConnnection (one-to-one mapping). Therefore, 4 server threads.
[-] bench the performance. see if it is 4 times as one thread. RESULT: nearly 4 times. when thread = 8, got 77% performance of one thread.
[-] use gperf or other tools to profile the program. DONE: nothing new. everywhere is not a hotspot. averagely spread to the Patronus APIs.
[-] bench the latency of QP recoverying. RESULT: 1 ms for BOTH server and client. matches the previous result I get.
[-] DONE: rename script to test_all.sh. support the correct.sh script. this should be easy. I have to fix all the correct_*.cpp files.
    - for example, correct_nomem is wrong.
    - remove all the unnecessary sleep. This kill the test phases.
[-] Add the header into the design. That would require adding the swaping area, and makes everything works well.
    - bind two mw, and return two mw to the user.
    - think about how user will use the meta area of the ProtectionRegion

Sorry I take one day off. Just too tired to work. Can not focus. 
2nd Tree days DDL: 1/14-1/16
[-] Won't Fix: See how to fix the frequency of the CPU. See if rdtsc() is stable. See if rdtsc() can map to real time (like ns).
    - Won't Fix: std::chrono already has great performance: 25 ns to get time_point, and a uint64_t-like performance.
    - and the std::chrono provides great API to compare and modify times.
    - Won't consider rdtsc.
[-] Won't Do: consider one dedicated core for gc
    - If I need Task manager accord threads, I actually need lock free map / skiplist / priority queue
    - Boost does not provide one, and folly doest. But folly is not buildable by clang (can workaround, but don't want to brother). Don't want to roll this proj back to using g++.
    - Also the latest clang (with -stdlib=libc++) can not build folly (which use boost, which use removed std::bind1st), so cannot use folly::coro.
    - It makes me reluctant to try folly's thread safe skip list. just too tired to try.
[-] Test the performance of std::chrono::system_clock? because Lease actually relies on real time...
    - great. system_clock has the same performance of steady_clock
[-] Implement a task_manager
    - could use single thread (per-thread) design, or a thread-safe queue design (let the master to do all the tasks)
[-] feat: add a TimeSyncer to patronus, which is used to sync the time at the begin of the system.
[-] DSM::get_rdma_buffer() return Buffer plz!
- Change the acquire lease API, and add some flags into it. server could react according to the flags.
    - so that, Server could know it is a bench_test (not automatically gc), or actual deploy (register gc task to the task manager).
    - so that, we could let server know whether it needs to speculatively binds a longer lease for one-sided extends.
- Add the TIME concepts into patronus. Server should intervally see if any expired lease exists, and destroy them!
    - IMPORTANT: what if server speculatively gc the resource, then client requires the gc the resource, i.e. the ABA problem? The resource is currently owns by others.
    - I think, should attach clientID to the on-going resource. If client requires to gc, see if 1) the resource is on-going (if gc-ed, will not do twice), and 2) the clientID matches (if not match, also skip.)
- Carefully thinks about lock conflict, fairness, client synchronizations at swap area, ...
- Implemente all the APIs in Patronus. Extend, upgrade would be the most important ones.

FUTURES:

If all the things above finish, I can move on to adopt the uPaxos!

- uPaxos
- some KVs
- motivation data
- micro evaluations